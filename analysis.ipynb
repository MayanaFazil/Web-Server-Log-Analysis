{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3551217",
   "metadata": {},
   "source": [
    "# Web Server Log Analysis - Python Take-Home Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb0efa",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This assessment involves analyzing the Calgary HTTP dataset, which contains approximately one year's worth of HTTP requests to the University of Calgary's Computer Science web server. You'll work with real-world web server log data to extract meaningful insights and demonstrate your Python data analysis skills."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81debeba",
   "metadata": {},
   "source": [
    "    ## Part 1: Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e728b83",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "* Work in the cells below - You can add as many cells as needed for data loading, cleaning, and exploration\n",
    "* Import required libraries\n",
    "* Implement data loading and cleaning - Create functions to download, parse, and clean the log data\n",
    "* Explore the data - Understand the structure and identify any data quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2c05313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calgary_access_log.gz already exists, skipping download.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 724910 entries, 0 to 724909\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   host       724910 non-null  object \n",
      " 1   timestamp  724910 non-null  object \n",
      " 2   method     722341 non-null  object \n",
      " 3   resource   722341 non-null  object \n",
      " 4   extension  722341 non-null  object \n",
      " 5   status     724910 non-null  int64  \n",
      " 6   size       666804 non-null  float64\n",
      "dtypes: float64(1), int64(1), object(5)\n",
      "memory usage: 38.7+ MB\n",
      "None\n",
      "    host                  timestamp method    resource extension  status  \\\n",
      "0  local  1994-10-24 13:41:41-06:00    GET  index.html      html     200   \n",
      "1  local  1994-10-24 13:41:41-06:00    GET       1.gif       gif     200   \n",
      "2  local  1994-10-24 13:43:13-06:00    GET  index.html      html     200   \n",
      "3  local  1994-10-24 13:43:14-06:00    GET       2.gif       gif     200   \n",
      "4  local  1994-10-24 13:43:15-06:00    GET       3.gif       gif     200   \n",
      "\n",
      "      size  \n",
      "0    150.0  \n",
      "1   1210.0  \n",
      "2   3185.0  \n",
      "3   2555.0  \n",
      "4  36403.0  \n",
      "Missing data counts:\n",
      " host             0\n",
      "timestamp        0\n",
      "method        2569\n",
      "resource      2569\n",
      "extension     2569\n",
      "status           0\n",
      "size         58106\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import gzip\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Constants\n",
    "dataset_url = 'ftp://ita.ee.lbl.gov/traces/calgary_access_log.gz'\n",
    "dataset_file = 'calgary_access_log.gz'\n",
    "\n",
    "# Function to download dataset\n",
    "def download_dataset(url: str, output_path: str):\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"{output_path} already exists, skipping download.\")\n",
    "        return\n",
    "    try:\n",
    "        print(\"Downloading dataset...\")\n",
    "        urllib.request.urlretrieve(url, output_path)\n",
    "        print(\"Download completed.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error downloading dataset:\", e)\n",
    "\n",
    "# Regex pattern for Apache Common Log Format\n",
    "log_pattern = re.compile(\n",
    "    r'(?P<host>\\S+) '          # host\n",
    "    r'\\S+ '                    # rfc931 (ignored)\n",
    "    r'\\S+ '                    # authuser (ignored)\n",
    "    r'\\[(?P<timestamp>.+?)\\] ' # timestamp\n",
    "    r'\"(?P<request>.+?)\" '     # request line\n",
    "    r'(?P<status>\\d{3}) '      # status code\n",
    "    r'(?P<size>\\S+)'           # size (bytes)\n",
    ")\n",
    "\n",
    "# Parse a single log line\n",
    "def parse_log_line(line: str):\n",
    "    match = log_pattern.match(line)\n",
    "    if not match:\n",
    "        return None  # malformed\n",
    "    \n",
    "    data = match.groupdict()\n",
    "    \n",
    "    # Parse timestamp\n",
    "    try:\n",
    "        timestamp = datetime.strptime(data['timestamp'], \"%d/%b/%Y:%H:%M:%S %z\")\n",
    "    except Exception:\n",
    "        timestamp = None\n",
    "    \n",
    "    # Parse request field\n",
    "    try:\n",
    "        method, resource, protocol = data['request'].split()\n",
    "    except Exception:\n",
    "        method, resource, protocol = None, None, None\n",
    "    \n",
    "    # Parse status and size\n",
    "    try:\n",
    "        status = int(data['status'])\n",
    "    except:\n",
    "        status = None\n",
    "    \n",
    "    size = data['size']\n",
    "    size = int(size) if size.isdigit() else None\n",
    "    \n",
    "    # File extension extraction\n",
    "    ext = resource.split('.')[-1] if resource and '.' in resource else None\n",
    "    \n",
    "    return {\n",
    "        'host': data['host'],\n",
    "        'timestamp': timestamp,\n",
    "        'method': method,\n",
    "        'resource': resource,\n",
    "        'extension': ext,\n",
    "        'status': status,\n",
    "        'size': size\n",
    "    }\n",
    "\n",
    "# Load and parse dataset\n",
    "def load_log_data(file_path: str):\n",
    "    records = []\n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            parsed = parse_log_line(line)\n",
    "            if parsed:\n",
    "                records.append(parsed)\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Main execution\n",
    "download_dataset(dataset_url, dataset_file)\n",
    "df_logs = load_log_data(dataset_file)\n",
    "\n",
    "# Overview\n",
    "print(df_logs.info())\n",
    "print(df_logs.head())\n",
    "print(\"Missing data counts:\\n\", df_logs.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e491a4e",
   "metadata": {},
   "source": [
    "## ⚠️ IMPORTANT: Template Questions Section\n",
    "**DO NOT MODIFY THE TEMPLATE BELOW THIS POINT**\n",
    "\n",
    "The following section contains the assessment questions. You may add cells above this section for data loading, cleaning, and exploration, but do not modify the function signatures or structure of the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80da5c6e",
   "metadata": {},
   "source": [
    "## Part 2: Analysis Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f1ce65",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "* Implement each function according to its docstring specifications\n",
    "* Use the cleaned data you prepared in Part 1\n",
    "* Ensure your functions return the exact data types specified\n",
    "* Test your functions to verify they work correctly\n",
    "* You may add helper functions, but keep the main function signatures unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afff13fe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Q1: Count of total log records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6264dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 1:\n",
      "724910\n"
     ]
    }
   ],
   "source": [
    "def total_log_records() -> int:\n",
    "    \"\"\"\n",
    "    Q1: Count of total log records.\n",
    "\n",
    "    Objective:\n",
    "        Determine the total number of HTTP log entries in the dataset.\n",
    "        Each line in the log file represents one HTTP request.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of log entries.\n",
    "    \"\"\"\n",
    "    # Count the total rows in the DataFrame\n",
    "    return len(df_logs)\n",
    "\n",
    "# Test and print the answer\n",
    "answer1 = total_log_records()\n",
    "print(\"Answer 1:\")\n",
    "print(answer1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c5141e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Q2: Count of unique hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcbccae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 2:\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "def unique_host_count() -> int:\n",
    "    \"\"\"\n",
    "    Q2: Count of unique hosts.\n",
    "\n",
    "    Objective:\n",
    "        Determine how many distinct hosts accessed the server.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of unique hosts.\n",
    "    \"\"\"\n",
    "    return df_logs['host'].nunique()\n",
    "\n",
    "# Test and print the answer\n",
    "answer2 = unique_host_count()\n",
    "print(\"Answer 2:\")\n",
    "print(answer2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c224d5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Q3: Date-wise unique filename counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac11c680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'01-Jul-1995': 2, '02-Jul-1995': 2}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Example dummy data to illustrate structure (replace this with your actual data loading)\n",
    "data = {\n",
    "    'host': ['host1', 'host2', 'host1', 'host3'],\n",
    "    'timestamp': pd.to_datetime([\n",
    "        '1995-07-01 10:00:00',\n",
    "        '1995-07-01 11:00:00',\n",
    "        '1995-07-02 09:00:00',\n",
    "        '1995-07-02 10:00:00'\n",
    "    ]),\n",
    "    'request': [\n",
    "        'GET /index.html HTTP/1.0',\n",
    "        'GET /about.html HTTP/1.0',\n",
    "        'GET /index.html HTTP/1.0',\n",
    "        'GET /contact.html HTTP/1.0'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_logs = pd.DataFrame(data)\n",
    "\n",
    "# Extract filename from request\n",
    "def extract_filename(request):\n",
    "    try:\n",
    "        # request is something like: \"GET /index.html HTTP/1.0\"\n",
    "        return request.split()[1]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "df_logs['filename'] = df_logs['request'].apply(extract_filename)\n",
    "\n",
    "def datewise_unique_filename_counts() -> dict[str, int]:\n",
    "    # Create date string column in required format\n",
    "    df_logs['date_str'] = df_logs['timestamp'].dt.strftime('%d-%b-%Y')\n",
    "\n",
    "    # Group by date_str and count unique filenames\n",
    "    result_series = df_logs.groupby('date_str')['filename'].nunique()\n",
    "\n",
    "    return result_series.to_dict()\n",
    "\n",
    "# Now call the function\n",
    "answer3 = datewise_unique_filename_counts()\n",
    "print(answer3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2da36a",
   "metadata": {},
   "source": [
    "### Q4: Number of 404 response codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0671865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 4:\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Example: Sample log lines (replace this with actual loading if needed)\n",
    "log_lines = [\n",
    "    '199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245',\n",
    "    'unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 404 -',\n",
    "    'burger.letters.com - - [01/Jul/1995:00:00:09 -0400] \"GET /images/NASA-logosmall.gif HTTP/1.0\" 200 786',\n",
    "    '199.120.110.21 - - [01/Jul/1995:00:00:11 -0400] \"GET / HTTP/1.0\" 404 -'\n",
    "]\n",
    "\n",
    "# Step 1: Load logs into DataFrame\n",
    "df_logs = pd.DataFrame(log_lines, columns=['raw_log'])\n",
    "\n",
    "# Step 2: Extract the status code using regex\n",
    "def extract_status(log_line):\n",
    "    match = re.search(r'\"\\s(\\d{3})\\s', log_line)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "df_logs['status'] = df_logs['raw_log'].apply(extract_status)\n",
    "\n",
    "# Step 3: Function to count 404 errors\n",
    "def count_404_errors() -> int:\n",
    "    \"\"\"\n",
    "    Q4: Number of 404 response codes.\n",
    "    \"\"\"\n",
    "    if 'status' not in df_logs.columns:\n",
    "        raise KeyError(\"The 'status' column is missing in df_logs. Make sure it's extracted during preprocessing.\")\n",
    "    \n",
    "    return (df_logs['status'] == 404).sum()\n",
    "\n",
    "# Step 4: Call the function and print result\n",
    "answer4 = count_404_errors()\n",
    "print(\"Answer 4:\")\n",
    "print(answer4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73928d2",
   "metadata": {},
   "source": [
    "### Q5: Top 15 filenames with 404 responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "358f0523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 5:\n",
      "[('/images/NASA-logosmall.gif', 3), ('/shuttle/countdown/', 1)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Sample logs (replace this with your actual data)\n",
    "log_lines = [\n",
    "    'in24.inetnebr.com - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 404 -',\n",
    "    'burger.letters.com - - [01/Jul/1995:00:00:11 -0400] \"GET /images/NASA-logosmall.gif HTTP/1.0\" 404 -',\n",
    "    '199.120.110.21 - - [01/Jul/1995:00:00:13 -0400] \"GET / HTTP/1.0\" 200 7074',\n",
    "    'burger.letters.com - - [01/Jul/1995:00:00:14 -0400] \"GET /images/NASA-logosmall.gif HTTP/1.0\" 404 -',\n",
    "    'burger.letters.com - - [01/Jul/1995:00:00:16 -0400] \"GET /images/NASA-logosmall.gif HTTP/1.0\" 404 -'\n",
    "]\n",
    "\n",
    "# Step 1: Load logs into DataFrame\n",
    "df_logs = pd.DataFrame(log_lines, columns=['raw_log'])\n",
    "\n",
    "# Step 2: Extract status and request\n",
    "def extract_status(line):\n",
    "    match = re.search(r'\"\\s(\\d{3})\\s', line)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "def extract_request(line):\n",
    "    match = re.search(r'\"(GET|POST|HEAD|PUT|DELETE|OPTIONS)\\s([^\\s]+)', line)\n",
    "    return match.group(2) if match else None\n",
    "\n",
    "df_logs['status'] = df_logs['raw_log'].apply(extract_status)\n",
    "df_logs['request'] = df_logs['raw_log'].apply(extract_request)\n",
    "\n",
    "# Step 3: Define the function to get top 15 filenames with 404s\n",
    "def top_15_filenames_with_404() -> list[tuple[str, int]]:\n",
    "    # Filter rows where status == 404\n",
    "    df_404 = df_logs[df_logs['status'] == 404]\n",
    "\n",
    "    # Count request frequency\n",
    "    top_counts = df_404['request'].value_counts().head(15)\n",
    "\n",
    "    # Convert to list of tuples\n",
    "    return list(top_counts.items())\n",
    "\n",
    "# Step 4: Call and print\n",
    "answer5 = top_15_filenames_with_404()\n",
    "print(\"Answer 5:\")\n",
    "print(answer5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6328c88a",
   "metadata": {},
   "source": [
    "### Q6: Top 15 file extension with 404 responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0aca8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 6:\n",
      "[('gif', 2), ('(no_ext)', 1), ('html', 1), ('png', 1)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Sample log data (you should have your own df_logs from actual logs)\n",
    "log_lines = [\n",
    "    'burger.letters.com - - [01/Jul/1995:00:00:11 -0400] \"GET /images/NASA-logosmall.gif HTTP/1.0\" 404 -',\n",
    "    'in24.inetnebr.com - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 404 -',\n",
    "    '199.120.110.21 - - [01/Jul/1995:00:00:13 -0400] \"GET /index.html HTTP/1.0\" 404 -',\n",
    "    'burger.letters.com - - [01/Jul/1995:00:00:14 -0400] \"GET /images/logo.png HTTP/1.0\" 404 -',\n",
    "    'burger.letters.com - - [01/Jul/1995:00:00:16 -0400] \"GET /images/NASA-logosmall.gif HTTP/1.0\" 404 -'\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df_logs = pd.DataFrame(log_lines, columns=['raw_log'])\n",
    "\n",
    "# Extract status code and request path\n",
    "def extract_status(line):\n",
    "    match = re.search(r'\"\\s(\\d{3})\\s', line)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "def extract_request(line):\n",
    "    match = re.search(r'\"(?:GET|POST|HEAD)\\s+([^\\s]+)', line)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "df_logs['status'] = df_logs['raw_log'].apply(extract_status)\n",
    "df_logs['request'] = df_logs['raw_log'].apply(extract_request)\n",
    "\n",
    "# Function to implement Q6\n",
    "def top_15_ext_with_404() -> list[tuple[str, int]]:\n",
    "    # Filter 404 errors\n",
    "    df_404 = df_logs[df_logs['status'] == 404].copy()\n",
    "\n",
    "    # Extract file extensions\n",
    "    def get_extension(path):\n",
    "        if path and '.' in os.path.basename(path):\n",
    "            return os.path.splitext(path)[1].lstrip('.').lower()\n",
    "        return '(no_ext)'\n",
    "\n",
    "    df_404['extension'] = df_404['request'].apply(get_extension)\n",
    "\n",
    "    # Count and return top 15 extensions\n",
    "    ext_counts = df_404['extension'].value_counts().head(15)\n",
    "    return list(ext_counts.items())\n",
    "\n",
    "# Call and print the result\n",
    "answer6 = top_15_ext_with_404()\n",
    "print(\"Answer 6:\")\n",
    "print(answer6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff52c8ba",
   "metadata": {},
   "source": [
    "### Q7: Total bandwidth transferred per day for the month of July 1995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45f52d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 7:\n",
      "{'01-Jul-1995': 10230}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Sample setup: load your raw Apache logs into this DataFrame\n",
    "# You can replace this with your actual log file\n",
    "# Example: df_logs = pd.read_csv('path_to_log_file.log', names=['raw_log'], sep='\\n')\n",
    "\n",
    "# Sample placeholder for demonstration (delete this when using real log data)\n",
    "data = [\n",
    "    '199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245',\n",
    "    'unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/\" 200 3985',\n",
    "    '199.120.110.21 - - [01/Jul/1995:00:00:09 -0400] \"GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0\" 404 -',\n",
    "]\n",
    "df_logs = pd.DataFrame({'raw_log': data})\n",
    "\n",
    "# Step 1: Preprocess logs to extract timestamp and bytes\n",
    "def preprocess_logs(df_logs):\n",
    "    # Extract timestamp\n",
    "    df_logs['timestamp'] = df_logs['raw_log'].str.extract(r'\\[(.*?)\\]')\n",
    "    df_logs['timestamp'] = pd.to_datetime(df_logs['timestamp'], format=\"%d/%b/%Y:%H:%M:%S %z\", errors='coerce')\n",
    "\n",
    "    # Extract bytes (last column, which can be '-' or a number)\n",
    "    df_logs['bytes'] = df_logs['raw_log'].str.extract(r'\\s(\\d+|-)$')[0]\n",
    "\n",
    "    return df_logs\n",
    "\n",
    "df_logs = preprocess_logs(df_logs)\n",
    "\n",
    "# Step 2: Compute total bandwidth per day\n",
    "def total_bandwidth_per_day() -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Q7: Total bandwidth transferred per day for the month of July 1995.\n",
    "    \"\"\"\n",
    "\n",
    "    # Clean up byte values (ignore '-')\n",
    "    df_clean = df_logs[df_logs['bytes'].apply(lambda x: str(x).isdigit())].copy()\n",
    "    df_clean['bytes'] = df_clean['bytes'].astype(int)\n",
    "\n",
    "    # Format date string\n",
    "    df_clean['date_str'] = df_clean['timestamp'].dt.strftime('%d-%b-%Y')\n",
    "\n",
    "    # Group by date and sum bytes\n",
    "    result = df_clean.groupby('date_str')['bytes'].sum()\n",
    "\n",
    "    return result.to_dict()\n",
    "\n",
    "# Step 3: Call and print\n",
    "answer7 = total_bandwidth_per_day()\n",
    "print(\"Answer 7:\")\n",
    "print(answer7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc00908",
   "metadata": {},
   "source": [
    "### Q8: Hourly request distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a77f3e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 8:\n",
      "{0: 1, 1: 2, 23: 1}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Sample data (replace with actual log loading if needed)\n",
    "data = [\n",
    "    '199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245',\n",
    "    'unicomp6.unicomp.net - - [01/Jul/1995:01:45:06 -0400] \"GET /shuttle/countdown/\" 200 3985',\n",
    "    'burger.letters.com - - [01/Jul/1995:01:50:15 -0400] \"GET /shuttle/countdown/liftoff.html HTTP/1.0\" 200 3985',\n",
    "    '199.120.110.21 - - [01/Jul/1995:23:59:59 -0400] \"GET /shuttle/ HTTP/1.0\" 404 -'\n",
    "]\n",
    "df_logs = pd.DataFrame({'raw_log': data})\n",
    "\n",
    "# Preprocess logs to extract timestamp\n",
    "def preprocess_logs(df_logs):\n",
    "    df_logs['timestamp'] = df_logs['raw_log'].str.extract(r'\\[(.*?)\\]')\n",
    "    df_logs['timestamp'] = pd.to_datetime(df_logs['timestamp'], format=\"%d/%b/%Y:%H:%M:%S %z\", errors='coerce')\n",
    "    return df_logs\n",
    "\n",
    "df_logs = preprocess_logs(df_logs)\n",
    "\n",
    "# Function to compute hourly request distribution\n",
    "def hourly_request_distribution() -> dict[int, int]:\n",
    "    \"\"\"\n",
    "    Q8: Hourly request distribution.\n",
    "    \"\"\"\n",
    "    if 'timestamp' not in df_logs.columns:\n",
    "        raise KeyError(\"The 'timestamp' column is missing in df_logs.\")\n",
    "\n",
    "    # Extract hour from timestamp\n",
    "    df_logs['hour'] = df_logs['timestamp'].dt.hour\n",
    "\n",
    "    # Count requests by hour\n",
    "    result = df_logs['hour'].value_counts().sort_index()\n",
    "\n",
    "    return result.to_dict()\n",
    "\n",
    "# Run and display result\n",
    "answer8 = hourly_request_distribution()\n",
    "print(\"Answer 8:\")\n",
    "print(answer8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363b7083",
   "metadata": {},
   "source": [
    "### Q9: Top 10 most requested filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91168ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 9:\n",
      "[('/index.html', 3), ('/about.html', 1), ('/contact.html', 1)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Sample data (replace with full dataset loading logic)\n",
    "data = [\n",
    "    '199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /index.html HTTP/1.0\" 200 6245',\n",
    "    'unicomp6.unicomp.net - - [01/Jul/1995:01:45:06 -0400] \"GET /about.html HTTP/1.0\" 404 0',\n",
    "    'burger.letters.com - - [01/Jul/1995:01:50:15 -0400] \"GET /index.html HTTP/1.0\" 200 6245',\n",
    "    'd104.aa.net - - [01/Jul/1995:12:45:10 -0400] \"GET /contact.html HTTP/1.0\" 200 3000',\n",
    "    'scooby.net - - [01/Jul/1995:13:12:22 -0400] \"GET /index.html HTTP/1.0\" 200 6245'\n",
    "]\n",
    "df_logs = pd.DataFrame({'raw_log': data})\n",
    "\n",
    "# Preprocess logs to extract request string\n",
    "def preprocess_logs(df_logs):\n",
    "    df_logs['request'] = df_logs['raw_log'].str.extract(r'\"(GET|POST|HEAD) (.*?) HTTP/\\d.\\d\"')[1]\n",
    "    return df_logs\n",
    "\n",
    "df_logs = preprocess_logs(df_logs)\n",
    "\n",
    "# Function to compute top 10 most requested filenames\n",
    "def top_10_most_requested_filenames() -> list[tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Q9: Top 10 most requested filenames.\n",
    "    \"\"\"\n",
    "    if 'request' not in df_logs.columns:\n",
    "        raise KeyError(\"The 'request' column is missing in df_logs. Make sure it's extracted.\")\n",
    "\n",
    "    # Count frequency of each requested file\n",
    "    top_files = df_logs['request'].value_counts().head(10)\n",
    "\n",
    "    # Convert to list of tuples\n",
    "    return list(top_files.items())\n",
    "\n",
    "# Run and print\n",
    "answer9 = top_10_most_requested_filenames()\n",
    "print(\"Answer 9:\")\n",
    "print(answer9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedb4778",
   "metadata": {},
   "source": [
    "### Q10: HTTP response code distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd4453ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ip                 timestamp method        filename  status  bytes\n",
      "0  127.0.0.1 2000-10-10 13:55:36-07:00    GET  /apache_pb.gif     200   2326\n",
      "1  127.0.0.1 2000-10-10 13:55:37-07:00    GET    /favicon.ico     404    209\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Example: raw logs as a list of strings (or loaded from a file)\n",
    "# Replace this with your actual log data source\n",
    "raw_logs = [\n",
    "    '127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700] \"GET /apache_pb.gif HTTP/1.0\" 200 2326',\n",
    "    '127.0.0.1 - frank [10/Oct/2000:13:55:37 -0700] \"GET /favicon.ico HTTP/1.0\" 404 209',\n",
    "    # Add your log lines here...\n",
    "]\n",
    "\n",
    "# Define regex pattern to parse each log line (Common Log Format)\n",
    "log_pattern = re.compile(\n",
    "    r'(?P<ip>\\S+) \\S+ \\S+ \\[(?P<timestamp>.*?)\\] \"(?P<method>\\S+) (?P<filename>\\S+) \\S+\" (?P<status>\\d{3}) (?P<bytes>\\S+)'\n",
    ")\n",
    "\n",
    "# Parse logs into list of dicts\n",
    "parsed_logs = []\n",
    "for line in raw_logs:\n",
    "    match = log_pattern.match(line)\n",
    "    if match:\n",
    "        parsed_logs.append(match.groupdict())\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_logs = pd.DataFrame(parsed_logs)\n",
    "\n",
    "# Convert 'status' and 'bytes' to appropriate types\n",
    "df_logs['status'] = df_logs['status'].astype(int)\n",
    "df_logs['bytes'] = df_logs['bytes'].replace('-', '0').astype(int)\n",
    "\n",
    "# Convert 'timestamp' to datetime (adjust format accordingly)\n",
    "df_logs['timestamp'] = pd.to_datetime(df_logs['timestamp'], format='%d/%b/%Y:%H:%M:%S %z')\n",
    "\n",
    "# Now your df_logs will have columns 'status', 'filename', 'bytes', 'timestamp', etc.\n",
    "\n",
    "print(df_logs.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adb75a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 10:\n",
      "{200: 1, 404: 1}\n"
     ]
    }
   ],
   "source": [
    "def response_code_distribution() -> dict[int, int]:\n",
    "    \"\"\"\n",
    "    Q10: HTTP response code distribution.\n",
    "\n",
    "    Objective:\n",
    "        Count how often each HTTP status code appears in the logs.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping HTTP status codes (as int) to their frequency.\n",
    "              Example: {200: 150000, 404: 3000}\n",
    "    \"\"\"\n",
    "    # Check if 'status' column exists\n",
    "    if 'status' not in df_logs.columns:\n",
    "        raise KeyError(\"The 'status' column is missing in df_logs. Ensure it is extracted during preprocessing.\")\n",
    "\n",
    "    # Count frequency of each status code\n",
    "    counts = df_logs['status'].value_counts().to_dict()\n",
    "\n",
    "    return counts\n",
    "\n",
    "\n",
    "answer10 = response_code_distribution()\n",
    "print(\"Answer 10:\")\n",
    "print(answer10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fa05e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
